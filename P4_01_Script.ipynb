{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P4_01_Script.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tysonjohn015/P04_AI_Credit_Risk_Model/blob/main/P4_01_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6mlsuq6TKA9"
      },
      "source": [
        "# PROJET 4 - CONSTRUISEZ UN MODELE DE SCORING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJgDu1RZTKBA"
      },
      "source": [
        "La société financière \"Prêt à dépenser\" souhaite pouvoir utiliser un modèle de scoring l'aidant à prédire le risque de défaut de paiement d'un client ayant peu ou pas d'historique de prêt.  \n",
        "Le modèle devra permettre aux conseillers qui l'utilisent de comprendre les motifs de l'acceptation ou du rejet de la demande de prêt.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZnK8IFeTKA3"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 200, 'display.max_rows', None, 'display.max_columns', None)\n",
        "\n",
        "import sklearn\n",
        "import plotly\n",
        "from distutils.version import LooseVersion as version\n",
        "if version(sklearn.__version__) < version(\"0.24.1\"):\n",
        "    !pip install --upgrade plotly\n",
        "    print(\"Sorry, I need at least version 0.24.1 of sklearn.\")\n",
        "else:\n",
        "    from sklearn import set_config\n",
        "    set_config(display='diagram')\n",
        "if version(plotly.__version__) < version(\"4.14.3\"):\n",
        "    !pip install --upgrade scikit-learn\n",
        "    print(\"Sorry, I need at least version 4.14.3 of plotly\")\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, learning_curve\n",
        "from sklearn.metrics import plot_confusion_matrix, fbeta_score, make_scorer, accuracy_score, confusion_matrix, plot_roc_curve\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print('Imports terminés.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAd8WFI1TKBC"
      },
      "source": [
        "## 1. Exploration des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4wT5BQubQDL"
      },
      "source": [
        "Télécharger les données de kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtifbUrWbbF7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFZDXxrMbZLp"
      },
      "source": [
        "# install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Copy the stackoverflow data set locally.\n",
        "!kaggle datasets download -d tysonjohn/datadd -f application_train.csv\n",
        "# !kaggle datasets download -d tysonjohn/datadd -f application_test.csv\n",
        "# !kaggle datasets download -d tysonjohn/datadd -f bureau.csv\n",
        "# !kaggle datasets download -d tysonjohn/datadd -f bureau_balance.csv\n",
        "# !kaggle datasets download -d tysonjohn/datadd -f previous_application.csv\n",
        "# !kaggle datasets download -d tysonjohn/datadd -f POS_CASH_balance.csv\n",
        "# !kaggle datasets download -d tysonjohn/datadd -f installments_payments.csv\n",
        "# !kaggle datasets download -d tysonjohn/datadd -f credit_card_balance.csv\n",
        "\n",
        "!unzip application_train.csv.zip\n",
        "# !unzip application_test.csv.zip\n",
        "# !unzip bureau.csv.zip\n",
        "# !unzip bureau_balance.csv.zip\n",
        "# !unzip previous_application.csv.zip\n",
        "# !unzip POS_CASH_balance.csv.zip\n",
        "# !unzip installments_payments.csv.zip\n",
        "# !unzip credit_card_balance.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu4HMoJcTKBE"
      },
      "source": [
        "Les données sont réparties en plusieurs fichiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMc7eWTRTKBT"
      },
      "source": [
        "### 1.1 Quel(s) fichier(s) utiliser ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enSq6K8uTKBa"
      },
      "source": [
        "Sur les 9 fichiers fournis (hors celui détaillant les informations contenues dans les autres) :\n",
        "\n",
        "- **application_train.csv** contient des données relatives à l'emprunteur lui-même, et concernant son crédit, au sort de celui-ci (mis en défaut de paiement ou non). Puisque le modèle est destiné à évaluer le risque de défaut d'emprunteurs n'ayant pas d'historique de crédit, il est bien sûr à utiliser.\n",
        "\n",
        "- **application_test.csv** ne nous sera pas utile dans le présent cas, car il ne comporte pas d'étiquettes relatives au sort du prêt (défaut ou pas). On ne l'utilisera donc pas.\n",
        "\n",
        "- **POS_CASH_balance.csv** : ce fichier est relatif au prêt en cours et au prêt précédent. Les données ne seront donc pas disponibles pour un nouvel emprunteur : il n'est donc pas utile pour l'entraînement du modèle.\n",
        "\n",
        "- **bureau.csv** : contient des informations sur le prêt en cours, donc là encore, inutile vu notre objectif.\n",
        "\n",
        "- **bureau_balance.csv** : La description précise \"use this to join to CREDIT_BUREAU table\", qui n'apparaît pas dans la liste. On prendra donc l'hypothèse qu'il faut la joindre avec la table bureau. Cette dernière n'étant pas utilisée, la présente table ne sera pas non plus utile.\n",
        "\n",
        "- **credit_card_balance** : se réfère au prêt en cours et au prêt précédent, donc inutile\n",
        "\n",
        "- **installment_payments** : idem\n",
        "\n",
        "- **previous_application_csv**: idem \n",
        "\n",
        "- **sample_submission.csv** : la signification de ce ficher n'est pas évidente puisque toutes les valeurs TARGET sont à 0.5. Faute d'information sur son utilité, il ne sera pas utilisé.\n",
        "\n",
        "Au final, seules les données contenues dans application_train.csv seront utilisées pour l'entraînement du modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOZ3iPeaTKBe"
      },
      "source": [
        "application_train = pd.read_csv('application_train.csv')\n",
        "application_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXfmRamRTKBi"
      },
      "source": [
        "## 2. Nettoyage des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeuQ4nc0TKBl"
      },
      "source": [
        "Pour optimiser le pipeline pour une utilisation future, la phase de nettoyage séquentiel des données est divisée en deux activités qui s'améliorent au fur et à mesure que le processus progresse.\n",
        "\n",
        "- une fonction destinée à supprimer les variables éventuellement redondantes et à créer de nouvelles variables synthétiques\n",
        "\n",
        "- une fonction destinée à traiter les valeurs manquantes ou aberrantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ-QVnLGTKBr"
      },
      "source": [
        "En première approche, nous allons commencer par entraîner un modèle de classification simple de type arbre de décision, sur les données simplement nettoyées des valeurs vides ou aberrantes. On ne modifiera donc pas les variables elles-mêmes dans cette étape. Puis, après examen des premiers résulats, nous affinerons notre démarche en supprimant / créant des variables si nécessaire. C'est donc la fonction de traitement des valeurs manquantes / aberrantes qui va être créée en premier lieu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36eMf1jcTKBu"
      },
      "source": [
        "### 2.1 Recherche des anomalies de valeurs numériques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OrGMsINTKBx"
      },
      "source": [
        "Etudions les distributions pour les variables numériques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlaxOArnTKBy"
      },
      "source": [
        "data = application_train.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6bfzpEDTKB0"
      },
      "source": [
        "def plot_hist_data(df):\n",
        "    num_cols = [col for col in df.columns if data[col].dtype != 'object'] \n",
        "        \n",
        "    height = int(np.ceil(len(num_cols)/6))\n",
        "    fig_height = 3 * height\n",
        "    fig = plt.figure(figsize=(20,fig_height))\n",
        "    \n",
        "    for feat_idx, col in enumerate(num_cols):\n",
        "        ax = fig.add_subplot(height, 6, feat_idx+1)\n",
        "        ax.hist(df[col], bins=50)\n",
        "        ax.set_title(col)\n",
        "    fig.tight_layout(pad=4)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zu97TjVTKB3"
      },
      "source": [
        "def plot_box_data(df):\n",
        "    num_cols = [col for col in df.columns if data[col].dtype != 'object'] \n",
        "    \n",
        "    height = int(np.ceil(len(num_cols)/6))\n",
        "    fig_height = 3 * height\n",
        "    fig = plt.figure(figsize=(20,fig_height))\n",
        "    \n",
        "    for feat_idx, col in enumerate(num_cols):\n",
        "        ax = fig.add_subplot(height, 6, feat_idx+1)\n",
        "        ax.boxplot(df[col])\n",
        "        ax.set_title(col)\n",
        "    fig.tight_layout(pad=4)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGAquH2FTKB6"
      },
      "source": [
        "# plot_hist_data(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh7v_DNOTKB9"
      },
      "source": [
        "# plot_box_data(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWLxDfdyTKCB"
      },
      "source": [
        "#### AMT_INCOME_TOTAL\n",
        "On observe des valeurs irréalistes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tORvLZ-ZTKCD"
      },
      "source": [
        "px.box(data['AMT_INCOME_TOTAL'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY_OzctBTKCG"
      },
      "source": [
        "Pour écarter les valeurs aberrantes, on utilisera le Z-score, c'est-à-dire la distance à la moyenne divisée par l'écart-type.  \n",
        "\n",
        "Si Z-score > 3, la valeur peut être considérée comme un outlier puisqu'elle ne fait pas partie des 99,7% valeurs les plus proches de la moyenne.\n",
        "\n",
        "Remarquons toutefois qu'en valeur absolue, le nombre de cas potentiellement écartés n'est pas négligeable :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC2yA7pPTKCI"
      },
      "source": [
        "def z_score(array, threshold=3):\n",
        "    '''Return an array of boolean, True for each value in the array where Z-score >threshold.'''\n",
        "    mean = array.mean()\n",
        "    std = array.std()\n",
        "    return abs((array-mean))/std > threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CASfOOffTKCL"
      },
      "source": [
        "nb_outliers = data[z_score(data['AMT_INCOME_TOTAL'])].shape[0]\n",
        "pc_outliers = nb_outliers / data.shape[0]\n",
        "print(f'Concernant la variable AMT_INCOME_TOTAL, on retrouve {nb_outliers} outliers \\\n",
        "qui représentent {pc_outliers:.2%} des cas.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql66PF3QTKCO"
      },
      "source": [
        "On risque donc de se priver d'outliers qui ne sont pas forcément des valeurs aberrantes, mais simplement hors norme, et qui pourraient obérer l'efficacité du modèle pour les très hauts revenus. Examinons les 10 plus gros revenus de la liste des emprunteurs :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sw7dWo8TKCP"
      },
      "source": [
        "data.sort_values(by='AMT_INCOME_TOTAL', ascending=False).head(10).style.format({'AMT_INCOME_TOTAL':'{:,.0f}'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv5xEO31TKCT"
      },
      "source": [
        "Mais le modèle que nous devons concevoir est fait pour être généraliste et s'appliquer aux cas 'normaux', au sens propre du terme. C'est donc en connaissance de cause que nous écarterons les données avec un Z-score supérieur à 3.  \n",
        "\n",
        "Il faudra garder à l'esprit que le modèle risque donc de ne pas donner de bons résultats sur les cas extrêmes (très hauts revenus notamment), des cas qui nécessiteront sans doute une approche manuelle, ou un autre modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvhvG-sgTKCV"
      },
      "source": [
        "Les autres variables étant susceptibles d'être supprimées ou modifiées par la suite, on appliquera un traitement global avec le même seuil de Z-score à 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoOSxHalTKCW"
      },
      "source": [
        "#### Données en nombre de jours\n",
        "Convertissons-les temporairement en années écoulées (divison par -365) pour faciliter la vérification : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkPFtUDecv3u"
      },
      "source": [
        "px.box(data['DAYS_EMPLOYED'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo_EBzUATKCX"
      },
      "source": [
        "day_cols = [col for col in data.columns if \"DAYS_\" in col]\n",
        "(data[day_cols].describe().loc[['min', 'max']].transpose()/-365).style.format({'min':'{:,.2f}', 'max':'{:,.2f}'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xXFbL-uTKCZ"
      },
      "source": [
        "Le max de 1000 ans pour DAYS_EMPLOYED est visible sur l'histogramme (sous la valeur -365 000 environ) et correspond à un nombre non négligeable de lignes (on affiche cette fois-ci les valeurs d'origine, sans la division par -365):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp8U79fvTKCa"
      },
      "source": [
        "(data[day_cols])[data['DAYS_EMPLOYED']>0].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEdVs8asTKCj"
      },
      "source": [
        "Les valeurs sont toutes indentiques à 365 243 jours. Cette valeur aberrante et constante suggère un parti pris dans la saisie ou l'encodage, par exemple dans le cas où la durée dans l'emploi n'est pas connue ou parce que la zone n'est pas remplie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-X2xNQJTKCl"
      },
      "source": [
        "days_count_pos = data['DAYS_EMPLOYED'][data['DAYS_EMPLOYED']>0].count()\n",
        "print(f\"Nombre de lignes avec des valeurs aberrantes pour DAYS_EMPLOYED : {days_count_pos} soit {days_count_pos/data.shape[0]:.2%} du total\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA9riDZvTKCm"
      },
      "source": [
        "Le nombre de cas concernés est trop important pour supprimer purement et simplement les lignes correspondantes. Nous allons prendre le parti de transformer ces valeurs en la valeur réaliste la plus proche, soit 0. Aux yeux d'un banquier, ne pas avoir d'emploi est à peu près équivalent à démarrer dans un poste le jour même de la demande de prêt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HwA6GS7TKCn"
      },
      "source": [
        "Par ailleurs, des durées supérieures à l'âge seraient également aberrantes, vérifions si c'est le cas :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbyFSbHBTKCo"
      },
      "source": [
        "for col  in day_cols:\n",
        "    idx = data[data[col]<data['DAYS_BIRTH']].index\n",
        "    idx_exist = len(idx)\n",
        "    if idx_exist:\n",
        "        print(col, 'présente des valeurs aberrantes aux lignes :', list(idx))\n",
        "        for i in idx:\n",
        "            print(data.loc[i,[col, 'DAYS_BIRTH']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTZygPipTKCq"
      },
      "source": [
        "Mis à part cette erreur liée à un simple arrondi, les autres valeurs ne semblent pas aberrantes à cet égard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l0SEMguTKCr"
      },
      "source": [
        "On établit donc comme critères de valeur aberrantes :\n",
        "- les valeurs > 0 qui seront ramenées à 0\n",
        "- pour toutes les valeurs sauf DAYS_BIRTH, des valeurs inférieures à celle-ci (autrement dit des durées supérieures à l'âge), qui seront ramenées à la valeur de DAYS_BIRTH.\n",
        "\n",
        "On pourrait aussi fixer des limites hautes et basse à DAYS_BIRTH, mais il est possible que la législation bancaire américaine permette les emprunts par des mineurs ou des personnes de plus de 70 ans : faute d'information, et les données présentes ne montrant pas d'aberration, on ne modifiera donc pas cette variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT-y6b7gTKCr"
      },
      "source": [
        "data[data['DAYS_EMPLOYED']>0].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq1IsWEAs8fm"
      },
      "source": [
        "data.loc[[8, 11, 23, 266366],day_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nffikDnUTKCt"
      },
      "source": [
        "for col in day_cols:\n",
        "    data.loc[data[col]< data['DAYS_BIRTH'], col] = data['DAYS_BIRTH']\n",
        "    data.loc[data[col]>0, col] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX7XZ-EyTKCu"
      },
      "source": [
        "data.loc[[8, 11, 23, 266366],day_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkAls0dJTKCv"
      },
      "source": [
        "#### Valeurs quantitatives ou qualitatives nulles ou manquantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3ZkVlFYH8Ao"
      },
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.heatmap(data.isna(),cbar=False)\n",
        "plt.title(\"Répartition des valeurs vides\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3zRESePI8Gu"
      },
      "source": [
        "print(data.dtypes.value_counts())\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.pie(data.dtypes.value_counts().values,autopct=\"%1.2f%%\",labels=[str(types) for types in data.dtypes.value_counts().index])\n",
        "plt.title(\"Répartition des types dans le jeu de données.\")\n",
        "plt.ylabel(\"Type des données\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcbWjijaTKCw"
      },
      "source": [
        "Examinons les valeurs pour les variables qualitatives :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QOWYya5TKCw"
      },
      "source": [
        "for col in [col for col in data.columns if data[col].dtype == 'object']:\n",
        "    print(col,\":\", list(application_train[col].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD-prrPlTKCx"
      },
      "source": [
        "Les données correspondent à des historiques de prêt réellement accordés. Les éventuelles valeurs nulles constatées sont donc en principe \"réelles\" : soit parce qu'il s'agit de valeurs binaires (FLAGS), soit parce que l'information est effectivement égale à zéro (ex : montant du salaire) ou indisponible (valeur 'nan') dans le dossier. \n",
        "\n",
        "Il ne nous paraît pas possible dans ce dataset de procéder à une imputation, chaque dossier étant unique. Mais les modèles n'acceptant pas la valeur NaN, si une donnée n'est pas connue, elle sera considérée comme nulle ou vide (selon sa nature) lors de la normalisation.\n",
        "\n",
        "Précisons que le choix de remplacer les valeurs quantitatives NaN à zéro peut se discuter. D'un point de vue métier, il se justifie pour la plupart des colonnes : un revenu non déclaré est considéré par le banquier comme un revenu nul, une ancienneté nulle dans l'emploi est équivalente à ses yeux à une absence d'emploi.  \n",
        "\n",
        "En revanche, concernant la variable d'ancienneté du véhicule par exemple, le choix de mettre la variable NaN à zéro met à égalité l'emprunteur qui ne possède pas de voiture, et celui qui fait sa demande de prêt justement le jour où il vient de prendre possession d'un nouveau véhicule : ce qui correspond à des situations patrimoniales différentes. Toutefois, cette information est complétée par le booléen \"OWN_CAR_FLAG\", qui permet de faire la différence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-9suDtGTKCy"
      },
      "source": [
        "On complète `clean_values()` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK6hMnVhTKCy"
      },
      "source": [
        "def clean_values(df):\n",
        "    print('Nettoyage des lignes...')\n",
        "    \n",
        "    old_length = df.shape[0]\n",
        "    result = df.copy()\n",
        "    num_cols = [col for col in result.columns if result[col].dtype!='object']\n",
        "    cat_cols = [col for col in result.columns if col not in num_cols]\n",
        "    \n",
        "    high_z_score = result[num_cols].apply(z_score, axis=0, args=[80])\n",
        "    result.drop(high_z_score[high_z_score.any(axis=1)].index, inplace=True)\n",
        "    \n",
        "    days_cols = [col for col in result.columns if \"DAYS_\" in col]\n",
        "    for col in day_cols:\n",
        "        result.loc[result[col]< result['DAYS_BIRTH'], col] = result['DAYS_BIRTH']\n",
        "        result.loc[result[col]>0, col] = 0\n",
        "        \n",
        "    result[num_cols]=result[num_cols].fillna(0)\n",
        "    result[cat_cols]=result[cat_cols].fillna('Not specified')\n",
        "    \n",
        "    new_length = result.shape[0]\n",
        "    deleted_rows = old_length - new_length\n",
        "    print(f'{deleted_rows} lignes ont été supprimées soit {deleted_rows/old_length:.2%} du total.')\n",
        "  \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twr_fqJBTKCz"
      },
      "source": [
        "## 3. Premier essai d'entraînement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JX3zqEiTKCz"
      },
      "source": [
        "Dans un premier temps, on supprime les valeurs aberrantes, sans modifier les variables existantes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycw-_15QTKC0"
      },
      "source": [
        "data = clean_values(application_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0vWzlZMTKC0"
      },
      "source": [
        "Préalable : extraction des variables et des données cible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otmyRXkosSed"
      },
      "source": [
        "def repartition_dataset(data,target,test_size=0.2,random_state=0,stratify_target=False,under_sampling=False,over_sampling=False):\n",
        "    if stratify_target:\n",
        "        train_set,test_set = train_test_split(data,test_size=test_size,random_state=random_state,stratify=data[target])\n",
        "    else:\n",
        "        train_set,test_set = train_test_split(data,test_size=test_size,random_state=random_state)\n",
        "        \n",
        "    # test set\n",
        "    X_test = test_set.drop(target,axis=1)\n",
        "    y_test = test_set[[target]]\n",
        "    print(\"Test =>\\nX :\", X_test.shape, \"\\ny :\",y_test.shape)\n",
        "    if under_sampling :\n",
        "        train_set = fair_data(train_set,target)\n",
        "    X_train = train_set.drop(target,axis=1)\n",
        "    y_train = train_set[[target]]\n",
        "    \n",
        "    if over_sampling :\n",
        "        over = SMOTE(sampling_strategy=0.1)\n",
        "        under = RandomUnderSampler(sampling_strategy=0.5)\n",
        "        steps = [('o', over), ('u', under)]\n",
        "        pipeline = Pipeline(steps=steps)\n",
        "        X_train, y_train = pipeline.fit_resample(X_train, y_train)\n",
        "    \n",
        "    print(\"Train =>\\nX :\", X_train.shape, \"\\ny :\",y_train.shape)\n",
        "    \n",
        "    return X_train,X_test,y_train,y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO4TQGDbTKC1"
      },
      "source": [
        "features_cols = [col for col in data.columns if col not in ['SK_ID_CURR']]\n",
        "X = data[features_cols]\n",
        "y = data['TARGET']\n",
        "id = data['SK_ID_CURR']\n",
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_rLLiLkscgO"
      },
      "source": [
        "X_train,X_test,y_train,y_test = repartition_dataset(data[features_cols],\"TARGET\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1yZ_LP2TKC1"
      },
      "source": [
        "### 3.1 Choix de la métrique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xihdzLwvTKC2"
      },
      "source": [
        "Nous cherchons à établir un modèle de classification binaire. Le résultat peut donc être positif (cas du client qui est en défaut de paiement) ou négatif (le client rembourse normalement).\n",
        "\n",
        "Un faux positif est un emprunteur qui serait à tort estimé comme présentant un risque de défaut.  \n",
        "Un faux négatif est un emprunteur qui serait estimé à tort comme ne présentant pas de risque de défaut.\n",
        "\n",
        "L'objectif de l'établissement bancaire est de détecter le plus possible des cas positifs (sensibilité élevée), et aussi de minimiser le taux de faux négatifs : en effet, cela voudrait dire qu'elle accorde un prêt à un client qui ne le remboursera pas. \n",
        "\n",
        "Pour autant, le taux de faux positifs n'est pas à négliger non plus car il représente pour un établissement une perte d'opportunité, en la personne d'un emprunteur qui aurait été solvable et auquel il aurait été possible de vendre d'autres produits et services.\n",
        "\n",
        "Le taux de faux positifs est estimé par la précision (nombre total de vrais positifs divisé par le nombre total de positifs prédits). Le taux de faux négatifs est estimé par 1 - recall (le recall, ou sensibilité, étant égal au nombre de vrais positifs rapporté au nombre total de cas positifs).\n",
        "\n",
        "L'optimisation entre la précision et le recall est donnée par la la maximisation du **F1-score**. Il s'agit de la moyenne harmonique de ces deux valeurs. Toutefois ce score attribue le même poids à ces derniers. Or nous souhaitons quant à nous privilégier la limitation des faux négatifs.\n",
        "\n",
        "On utilisera donc le **F2-score** (c'est-à-dire un F-beta avec un Beta égal à 2), qui surpondère les faux négatifs ([source](https://machinelearningmastery.com/fbeta-measure-for-machine-learning/)).\n",
        "\n",
        ".  Precision : $\\frac{TP}{TP + FP}$\n",
        "\n",
        ".  Recall : $\\frac{TP}{TP + FN}$\n",
        "\n",
        ".  FBeta-score ($F_{\\beta}$) : $F_{\\beta} = (1 + \\beta^{2}) \\frac{precision * recall}{\\beta^{2} * precision + recall}$ \n",
        "\n",
        "\n",
        "\n",
        "> $TP$ => True Positif  \n",
        "> $FP$ => False Postif <br>\n",
        "> $FN$ => False Negatif"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndz2poF8TKC3"
      },
      "source": [
        "### 3.2 Base de comparaison : classifieur naïf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eMSrfxoHSlW"
      },
      "source": [
        "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
        "\n",
        "def evaluation(model,X_test,y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    matrix_pred_model(y_test,y_pred)\n",
        "    score = ftwo_scorer(model,X_test,y_test)\n",
        "    print(\"Score (fbeta 2):\",score )\n",
        "    return score\n",
        "\n",
        "def all_eval(model,X_test,y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    recall = recall_score(y_test,y_pred)\n",
        "    precision = accuracy_score(y_test,y_pred)\n",
        "    score = ftwo_scorer(model,X_test,y_test)\n",
        "    \n",
        "    print(classification_report(y_test,y_pred))\n",
        "    return recall, precision, score\n",
        "\n",
        "def matrix_pred_model(y_test,y_pred,figsize=(5,5)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.title(\"Matrice de confusion\")\n",
        "    sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt=\"d\",cbar=False)\n",
        "    plt.xlabel(\"Prédiction\")\n",
        "    plt.ylabel(\"Classe initial\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_all_roc_curve(all_model,X_test,y_test,figsize=(9,7),title_roc_curve=\"ROC Curve\",naive_model=None):\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    model_displays = {}\n",
        "    for name, pipeline in all_model.items():\n",
        "        model_displays[name] = plot_roc_curve(pipeline, X_test, y_test, ax=ax, name=name)\n",
        "        \n",
        "    if not naive_model is None:\n",
        "        model_displays[\"Naive\"] = plot_roc_curve(naive_model, X_test, y_test, ax=ax, name=\"Naive\")\n",
        "        \n",
        "    _ = ax.set_title(title_roc_curve)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-8nzmCPTKC3"
      },
      "source": [
        "On établit un dummy classifier qui servira de point de comparaison pour évaluer l'efficacité du modèle. Pour le choix du type de dummy classifier, vérifions la répartition des classes dans les données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjyW3IGVTKC3"
      },
      "source": [
        "sum(y)/len(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA6S0nCxTKC4"
      },
      "source": [
        "Comme on pouvait s'y attendre, les cas de défaut sont très minoritaires (8,07%). Un modèle qui prédirait systématiquement la classe négative (pas de défaut de paiement, l'emprunteur paie normalement) aura donc une accuracy de 91,97%.\n",
        "\n",
        "On choisit un Dummy Classifier qui renvoie des prédictions avec la même distribution que le jeu d'entraînement, afin d'avoir la même sensibilité (taux de vrais positifs) et la même spécificité (taux de vrais négatifs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsmwyu2bYi58"
      },
      "source": [
        "## NAIVE\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
        "dummy_clf.fit(X_train,y_train)\n",
        "\n",
        "f2_dum = evaluation(dummy_clf,X_test,y_test)\n",
        "acc_dum = dummy_clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P00KgJU5TKC5"
      },
      "source": [
        "# dum = DummyClassifier(strategy='stratified')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tKMPZJpTKC5"
      },
      "source": [
        "# _ = dum.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRMM04L7TKC6"
      },
      "source": [
        "L'accuracy de ce classifieur est de :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQkHfUe1TKC6"
      },
      "source": [
        "# acc_dum = dum.score(X, y)\n",
        "# acc_dum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLHUYinTTKC7"
      },
      "source": [
        "Et son F2 score est de :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2OTCLdnTKC8"
      },
      "source": [
        "# y_pred_dum = dum.predict(X)\n",
        "# f2_dum = fbeta_score(y, y_pred_dum, beta=2)\n",
        "# f2_dum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__cLeHP8TKC8"
      },
      "source": [
        "Notre modèle doit donc surperformer ce classifieur naïf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuCR8P1qTKC9"
      },
      "source": [
        "Construisons un tableau récapitulatif pour suivre l'évolution des scores selon les progrès de la modélisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q84O-L8TKC9"
      },
      "source": [
        "recap = pd.DataFrame(columns=['accuracy', 'F2-score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpD2LYa8TKC-"
      },
      "source": [
        "def update_recap(index, acc, f2, recap=recap):\n",
        "    '''Add line to recap ith acuracy and f2 score.'''\n",
        "    if index not in recap.index:\n",
        "        recap.loc[index] = [acc, f2]\n",
        "    return recap.style.format('{:.2%}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihBlm4VgTKC-"
      },
      "source": [
        "update_recap('dummy', acc_dum, f2_dum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMr6twLeTKC_"
      },
      "source": [
        "### 3.3 Standardisation et encodage des valeurs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tv0JBkfTKC_"
      },
      "source": [
        "La transformation des données présente deux contraintes :  \n",
        "* d'une part, les données sont de nature différente (quantitative ou qualitative), et parmi les données quantitatives, certaines sont déjà normalisées ou binaires, elles nécessitent donc des traitements différents (standardisation - ou pas - ou encodage);  \n",
        "* d'autre part, la déséquilibre des classes rend d'autant plus nécessaire la séparation des jeux de test, et le recours à un stratified k-fold. Or, pour éviter toute fuite d'information, la séparation des jeux de données doit être effectuée avant la normalisation.\n",
        "    \n",
        "Le recours à un pipeline va permettre d'effectuer la normalisation après chaque stratification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqnvtwFBTKDA"
      },
      "source": [
        "La standardisation sera réalisée avec StandardScaler, l'encodage par One Hot Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-06-10T12:48:22.048348Z",
          "iopub.execute_input": "2021-06-10T12:48:22.048699Z",
          "iopub.status.idle": "2021-06-10T12:48:22.060697Z",
          "shell.execute_reply.started": "2021-06-10T12:48:22.048671Z",
          "shell.execute_reply": "2021-06-10T12:48:22.058927Z"
        },
        "trusted": true,
        "id": "Gtr9WoJITKDA"
      },
      "source": [
        "num_cols = [col for col in X_train.columns if data[col].dtype != 'object']\n",
        "cat_cols = [col for col in X_train.columns if col not in num_cols]\n",
        "\n",
        "transformer = make_column_transformer(\n",
        "    (StandardScaler(), num_cols),\n",
        "    (OneHotEncoder(), cat_cols)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcccOQxOTKDA"
      },
      "source": [
        "On vérifie que le transformer produit le résultat attendu (sans modifier X) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-06-10T12:48:22.062339Z",
          "iopub.execute_input": "2021-06-10T12:48:22.062659Z",
          "iopub.status.idle": "2021-06-10T12:48:37.847813Z",
          "shell.execute_reply.started": "2021-06-10T12:48:22.062629Z",
          "shell.execute_reply": "2021-06-10T12:48:37.846660Z"
        },
        "trusted": true,
        "id": "hZ6leFgwTKDB"
      },
      "source": [
        "result = transformer.fit_transform(X_train)\n",
        "transformed_cols = num_cols + list(transformer.named_transformers_['onehotencoder'].get_feature_names())\n",
        "df_result = pd.DataFrame(result, columns=transformed_cols)\n",
        "print(df_result.shape)\n",
        "df_result.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7a4l4bfIdTr"
      },
      "source": [
        "df_result.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_IQzf1BTKDC"
      },
      "source": [
        "Les données sont prêtes à être fournies au pipeline du modèle. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZJOMpyGTKDC"
      },
      "source": [
        "### 3.4 Entraînement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40AYZHsKTKDD"
      },
      "source": [
        "Pour pouvoir utiliser une stratégie d'optimisation KStratifiedDFolds, on utilisera la fonction GridSearchCV de sckit-learn. Elle permet :\n",
        "-  d'éviter une fuite d'information entre jeux d'entraînement et de test, en les séparant avant normalisation\n",
        "- d'entraîner le modèle, pour pouvoir ensuite examiner les résultats (là où une simple cross_validation ne ferait que donner les scores pour chaque kfold).\n",
        "\n",
        "De plus, elle sera également applicable pour la sélection des hyperparamètres (qui est sa fonction première). Enfin, elle permet la parallélisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCECbo2XTKDD"
      },
      "source": [
        "Commençons avec un DecisionTreeClassifier avec les données simplement nettoyées des valeurs aberrantes, et avec les hypermaramètres par défaut :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkh9JBCHTKDD"
      },
      "source": [
        "pipeline = make_pipeline(transformer, DecisionTreeClassifier())\n",
        "params = {'decisiontreeclassifier__max_depth': [None]}\n",
        "\n",
        "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
        "accuracy_scorer = make_scorer(accuracy_score)\n",
        "\n",
        "grid_dtc = GridSearchCV(\n",
        "    pipeline, \n",
        "    param_grid=params, \n",
        "    cv=5, \n",
        "    scoring={'acc': accuracy_scorer, 'ftwo': ftwo_scorer},\n",
        "    refit='ftwo', \n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAs182JITKDE"
      },
      "source": [
        "_ = grid_dtc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXjrKVpWTKDF"
      },
      "source": [
        "# best_estimator = grid_dtc.best_estimator_\n",
        "# _ = best_estimator.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhKCuZ29zet0"
      },
      "source": [
        "f2_dtc = evaluation(grid_dtc,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg6ZmhmWYAAH"
      },
      "source": [
        "plot_all_roc_curve({\"Decision Tree\":grid_dtc},X_test,y_test,naive_model=dummy_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbx0N6wO0E6Y"
      },
      "source": [
        "# update_recap('dtc', acc_dtc, f2_dtc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbxZJ40tTKDF"
      },
      "source": [
        "# plt.figure(figsize=(20,16)) \n",
        "# _=plot_tree(best_estimator.named_steps['decisiontreeclassifier'], \n",
        "#             max_depth=4, feature_names=transformed_cols, \n",
        "#             proportion=True, filled=True, fontsize=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja9cjdoqTKDG"
      },
      "source": [
        "On remarque que ce sont les éléments 'EXT_SOURCE' qui semblent les plus significatifs. Mais les résultats, bien que meilleurs que ceux du classifieur naïf, ne sont pas bons pour autant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3tLva-STKDG"
      },
      "source": [
        "update_recap('dtc basique', grid_dtc.cv_results_['mean_test_acc'][0], grid_dtc.cv_results_['mean_test_ftwo'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHd4Xim2TKDG"
      },
      "source": [
        "## 4. Amélioration du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9AV709jTKDH"
      },
      "source": [
        "- modification des variables\n",
        "- modification des hyperparamètres\n",
        "- changement de modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_k6NaDFTKDH"
      },
      "source": [
        "### 4.1 Amélioration des variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMsbllmTTKDI"
      },
      "source": [
        "Quelques constats préliminaires :\n",
        "\n",
        "- le revenu ne peut être que d'une seule catégorie, alors qu'il est tout à fait possible d'avoir à la fois un salaire et des revenus immobiliers par exemple.\n",
        "- la variable FLAG_WORK_PHONE est indiquée comme correspondant à l'existence du numéro de téléphone du domicile dans le dossier, alors que le nom est plus évocateur d'un numéro professionnel, d'autant qu'il existe aussi la variable FLAG_PHONE pour le domicile.\n",
        "- il existe deux variables liées au numéro professionnel : FLAG_WORK_PHONE et FLAG_EMP_PHONE. Peut-être sont-elles différenciées pour traiter séparément les cas d'un entrepreneur indépendant / profession libérale et les cas de salariés, ou pour différencier la ligne directe professionnelle de celle de l'établissement, mais cela semble une lourdeur inutile.\n",
        "- la variable FLAG_MOBIL doublonne avec FLAG_CONT_MOBIL : l'important n'est pas d'avoir donné un numéro mais qu'il soit effectivement joignable.\n",
        "- Les variables EXT_SOURCE et FLAG_DOCUMENT ne permettent pas de vérifier que les sources et documents en question sont toujours fournis dans l'ordre attendu. Il conviendrait de vérifier ce point auprès de l'établissement bancaire. Si celui-ci n'est pas en mesure de certifier que c'est bien le cas, ces variables pourraient très certainement être regroupées.\n",
        "- bien que la magie de la Data Science soit justement de faire ressortir des corrélations cachées, on ne peut s'empêcher de se demander dans quelle mesure la nature des matériaux de construction, le nombre d'entrées ou d'étages du logement de l'emprunteur, pourraient conditionner sa capacité à rembourser le prêt.\n",
        "- les données relatives au cercle relationnel du client donnent à penser qu'il est licite et courant de demander ce type d'information aux USA, alors qu'en France cela constituerait une infraction à la fois au secret bancaire et au RGPD. Ce qui pose au passage la question de l'exportabilité d'un modèle de ML d'un pays à l'autre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNz2RgzCTKDI"
      },
      "source": [
        "Examinons les corrélations entre les variables :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kKQxSeETKDJ"
      },
      "source": [
        "def show_correlation_matrix(corr):\n",
        "    '''display correlation matrix.'''\n",
        "    fig = px.imshow(corr, title = f\"Corrélation entre les variables\", \n",
        "                    labels={'color':\"Corrélation\"}, \n",
        "                    color_continuous_scale='RdBu',\n",
        "                    color_continuous_midpoint=0,\n",
        "                    width=1100, height=1100, )\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq8fMX5WTKDJ"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ql67IcNTKDK"
      },
      "source": [
        "corr = data.corr()\n",
        "show_correlation_matrix(corr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQhQeVnHTKDK"
      },
      "source": [
        "On remarque que certaines corrélations sont à NaN, ce qui correspond à des variables qui ont une seule valeur pour toutes les données (toutes ces variables sont binaires) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvTzcNXGTKDK"
      },
      "source": [
        "data[list(corr[corr['AMT_INCOME_TOTAL'].isna()].index)].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsba7aFpTKDL"
      },
      "source": [
        "Ceci est dû à la suppression des lignes présentant des outliers dans d'autres catégories. Ces variables ne sont donc d'aucune utilité pour entraîner le modèle et peuvent être supprimées."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yFx-N5lTKDL"
      },
      "source": [
        "constant_cols = corr[corr['AMT_INCOME_TOTAL'].isna()].index.to_list()\n",
        "data.drop(constant_cols, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxzpE1jMTKDM"
      },
      "source": [
        "show_correlation_matrix(data.corr())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhJ8LBz8TKDM"
      },
      "source": [
        "Remarquons ensuite que le résultat (TARGET) ne semble nettement corrélé linéairement à aucune variable en particulier. Voici ci-après les corrélations, positives ou négatives, classées par ordre décroissant de valeur absolue :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQjC6MD3TKDN"
      },
      "source": [
        "pd.DataFrame(corr['TARGET'].sort_values(ascending=False, key=lambda x: np.abs(x))).head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8uLqmC1TKDN"
      },
      "source": [
        "Les données relatives au logement sont fortement corrélées entre elles et il semble donc possible de les synthétiser. Dans un premier temps, on peut déjà supprimer le mode et la médiane pour ne conserver que la moyenne. Cette suppression sera intégrée à la fonction `clean_features()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNRZoHMtTKDO"
      },
      "source": [
        "columns_without_mode_and_medi = [col for col in data.columns if not (col.endswith('_MODE') or col.endswith('_MEDI'))]\n",
        "data = data[columns_without_mode_and_medi]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyR4lpzBTKDP"
      },
      "source": [
        "Revisualisons la matrice de corrélation après suppression de ces éléments :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxgKAMi_TKDP"
      },
      "source": [
        "show_correlation_matrix(data.corr())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H38M_F2QTKDP"
      },
      "source": [
        "Maintenant que le nombre de variables a été sensiblement réduit, examinons les corrélations supérieures (en valeur absolue) à 0.7 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttg8anIQTKDQ"
      },
      "source": [
        "def list_high_correlations(df, threshold):\n",
        "    '''Print pairs of correlated features in corr and their correlation score.'''\n",
        "    corr = df.corr()\n",
        "    limit = 0\n",
        "    list_of_corr = []\n",
        "    for row in corr.index:\n",
        "        for col in corr.columns[limit:]:\n",
        "            if threshold<np.abs(corr.loc[row, col])<1:\n",
        "                list_of_corr.append(((row, col), corr.loc[row, col]))\n",
        "        limit+=1\n",
        "        list_of_corr = sorted(list_of_corr, key= lambda x: np.abs(x[1]), reverse=True)\n",
        "    if not list_of_corr:\n",
        "        print(f'Aucune corrélation supérieure à {threshold}.')\n",
        "    else:\n",
        "        for element in list_of_corr:\n",
        "            print(element[0], \":\", f'{element[1]:.2%}')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxXJ2NhITKDQ"
      },
      "source": [
        "list_high_correlations(data, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agv_Cl9MTKDR"
      },
      "source": [
        "Décidons maintenant du traitement de ces variables. Le coefficient de corrélation est arrondi à 2 décimales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMMrguDSTKDS"
      },
      "source": [
        "**('DAYS_EMPLOYED', 'FLAG_EMP_PHONE') : -1**\n",
        "\n",
        "La corrélation inverse, arrondie, est totale. Ce qui semble logique puisque dès lors que le demandeur de prêt a un employeur, il pourra fournir son numéro. Cette dernière information est donc en soi redondante (elle correspond peu ou prou à \"a un employeur\") et **la variable FLAG_EMP_PHONE peut être supprimée**, tandis que la variable concernant l'ancienneté dans l'emploi est beaucoup plus intéressante (c'est une variable attentivement étudiée par un banquier)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv2CrH5BTKDS"
      },
      "source": [
        "data.drop(['FLAG_EMP_PHONE'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ5t6ajwTKDT"
      },
      "source": [
        "**('OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE') : 1**  \n",
        "**('DEF_30_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE') : 0.86**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77KQbt5BTKDT"
      },
      "source": [
        "Traçons le nuage de points pour les retards de paiement (pour une meilleure lisibilité, on ignorera la valeur aberrante de 347, on reviendra sur cette valeur dans la 2ème partie du traitement des données)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCgcwb0LTKDT"
      },
      "source": [
        "px.scatter(data[data['OBS_30_CNT_SOCIAL_CIRCLE']<50], x='OBS_30_CNT_SOCIAL_CIRCLE', y='OBS_60_CNT_SOCIAL_CIRCLE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noSw75QjTKDU"
      },
      "source": [
        "La corrélation étant parfaite, on peut choisir d'ignorer une des deux variables. Reste à savoir laquelle. Etudions les variables relatives aux défaut de paiement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwOayuBjTKDU"
      },
      "source": [
        "px.scatter(data[data['DEF_30_CNT_SOCIAL_CIRCLE']<50], x='DEF_30_CNT_SOCIAL_CIRCLE', y='DEF_60_CNT_SOCIAL_CIRCLE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4pVs7V-TKDV"
      },
      "source": [
        "Reprenons les définitions de DEF_30 et DEF_60 : \"How many observation of client's social surroundings defaulted on [30]/[60] (days past due) DPD\". Cette variable recense le nombre de défauts de paiement dans l'entourage de l'emprunteur suite à un retard de paiement de 30 ou 60 jours. On constate sur le graphe que DEF60 n'est jamais supérieur à DEF 30, ce qui est logique : le nombre de personnes en défaut de paiement depuis plus d'un mois comprend celui en défaut de paiement depuis plus de deux mois (mais pas l'inverse).\n",
        "\n",
        "On va donc choisir de retenir DEF_30, et par conséquent OBS_30 également. On supprimera donc les variables DEF_60 et OBS_60."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlcME35jTKDW"
      },
      "source": [
        "data.drop(['OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNC_2sCiTKDW"
      },
      "source": [
        "**('AMT_CREDIT', 'AMT_ANNUITY') 0.77**\n",
        "\n",
        "Le montant de l'annuité croît linéairement dans le même sens que celui du crédit bien sûr, mais il dépend également de deux autres facteurs : le taux et la durée du prêt. Or ces deux facteurs pourraient aboutir à un paradoxe de Simpson. En effet, le taux du prêt est décidé par le banquier en fonction de plusieurs paramètres, dont la durée du prêt et la motivation de l'établissement bancaire à accorder le prêt. Le taux du prêt dépend donc de la qualité du dossier de l'emprunteur aux yeux de la banque. Or, pour une banque, un emprunteur de qualité, c'est un emprunteur qui ne fait pas défaut : c'est justement l'objectif du modèle que de le prédire ! \n",
        "\n",
        "Quant à la durée du prêt, elle impacte très fortement le montant de l'échéance et joue à ce titre un rôle majeur dans l'endettement. Il suffit parfois de la moduler pour que l'endettement passe d'excessif à acceptable.\n",
        "\n",
        "Les banques françaises utilisent couramment deux autres variables pour mesurer le risque client : le taux d'endettement (rapport entre le total des échéances à payer et les revenus) et le reste à vivre (différence entre les échéances à payer, majorées de charges fixe de type loyer, et les revenus). Mais cela nécessite de commencer par calculer le montant de l'échéance du prêt envisagé, donc à refaire tourner le modèle pour chaque durée considérée. \n",
        "\n",
        "Une variable plus pertinente dans ce cas est le **rapport entre le montant du crédit et le total des revenus** : il donne une idée, certes approximative, de la mesure dans laquelle le bien est dans les moyens de l'acquéreur. Par exemple, si on retient une limite d'endettement de 33%, cela signifie qu'un prêt remboursé sans encombre pendant 15 ans représentait environ 5 ans de salaire (hypothèse très simplifiée ici puisqu'on ne tient pas compte du taux du prêt). Acheter un bien qui vaut 5 ans de salaire est donc une opération raisonnablement envisageable. Par contre, acheter un bien représentant 25 ans de salaire impliquerait de devoir s'endetter à hauteur de 33% de ses revenus pendant plus de 75 ans.  \n",
        "\n",
        "**On créera donc la variable \"CREDIT_INCOME_RATIO\" et on supprimera la variable AMT_ANNUITY** qui est non seulement inutile mais dangereuse pour la fiabilité du modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vU_1iExTKDX"
      },
      "source": [
        "data['CREDIT_INCOME_RATIO'] = data['AMT_CREDIT'] / data['AMT_INCOME_TOTAL']\n",
        "data.drop(['AMT_ANNUITY'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vychGkQ8TKDX"
      },
      "source": [
        "**('AMT_CREDIT', 'AMT_GOODS_PRICE') 0.99**  \n",
        "**('AMT_ANNUITY', 'AMT_GOODS_PRICE') : 0.78**\n",
        "\n",
        "Là encore, la corrélation parfaite observée  entre le montant du crédit et celui du bien est évidente et a beaucoup moins d'intérêt que le **ratio** entre le montant du crédit et celui de la valeur du bien. C'est le ratio d'apport personnel, également scruté de près par les banquiers, du moins en France. Couplé à CREDIT_INCOME_RATIO, il donne là aussi une bonne idée de la faisabilité du dossier. On créera donc la variable **'CREDIT_GOOD_RATIO' qui sera égale à AMT_CREDIT / AMT_GOODS_PRICE, et on supprimera la variable 'AMT_CREDIT'** puisqu'elle corrélée à la fois a CREDIT_GOOD_RATIO et à CREDIT_INCOME_RATIO.\n",
        "\n",
        "Comme vu plus haut, la varable AMT_ANNUITY est supprimée, et on pourra également **supprimer la valeur d'achat du bien AMT_GOODS_PRICE**, puisqu'elle peut être reconstituée à partir de AMT_INCOME, CREDIT_INCOME_RATIO et CREDIT_GOOD_RATIO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTgRlNEYTKDY"
      },
      "source": [
        "data['CREDIT_GOOD_RATIO'] = data['AMT_CREDIT']/(data['AMT_GOODS_PRICE']+1)\n",
        "data.drop(['AMT_CREDIT', 'AMT_GOODS_PRICE'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U_135DoTKDY"
      },
      "source": [
        "**('REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY') 0.95**\n",
        "\n",
        "La forte corrélation s'explique par le fait que l'opinion d'un établissement sur la région dont provient le client est influencée, notamment, par les villes qui en font partie. L'exclusion de la ville ne semble donc pas un choix significatif et la variable correspondante, **REGION_RATING_CLIENT_W_CITY, paraît pouvoir être omise dans le modèle**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptdWh9qtTKDZ"
      },
      "source": [
        "data.drop(['REGION_RATING_CLIENT_W_CITY'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mz2Hc8pTKDa"
      },
      "source": [
        "**('APARTMENTS_AVG', 'ELEVATORS_AVG') 0.84  \n",
        "('APARTMENTS_AVG', 'LIVINGAPARTMENTS_AVG') 0.94  \n",
        "('APARTMENTS_AVG', 'LIVINGAREA_AVG') 0.91  \n",
        "('ELEVATORS_AVG', 'LIVINGAPARTMENTS_AVG') 0.81  \n",
        "('ELEVATORS_AVG', 'LIVINGAREA_AVG') 0.87**  \n",
        "On constate que la variable APARTMENTS_AVG présente des corrélations élevées avec les variables ELEVATORS_AVG, LIVINGAPARTMENTS_AVG et LIVING_AREA_AVG : on peut donc supprimer ces dernières. En pratique, on peut d'ailleurs douter de la pertinence, voire de l'utilité de ces éléments dont on ne connaît pas le mode de détermination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHqty1hqTKDa"
      },
      "source": [
        "data = data.drop(['ELEVATORS_AVG', 'LIVINGAPARTMENTS_AVG','LIVINGAREA_AVG'], \n",
        "                       axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIZu7Y-aTKDb"
      },
      "source": [
        "**('CNT_CHILDREN', 'CNT_FAM_MEMBERS') 0.88**\n",
        "\n",
        "La corrélation entre le nombre d'enfants et le nombre de membres de la famille n'est pas surprenante, puisque les enfants forment un sous-ensemble des membres de la famille. Examinons-la :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHG6f8uJTKDb"
      },
      "source": [
        "px.scatter(data, x=data['CNT_CHILDREN'], y=data['CNT_FAM_MEMBERS'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StPQ8BMhTKDb"
      },
      "source": [
        "Le nombre de membres de la famille correspond donc au(x) parent(s) du foyer, et c'est la statistique avec le nombre d'enfants qui est la plus pertinente. L'existence d'un lien de couple étant déjà traité par la donnée NAME_FAMILY_STATUS, la variable CNT_FAMILY_MEMBER peut être supprimée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81KJOswhTKDc"
      },
      "source": [
        "data.drop(['CNT_FAM_MEMBERS'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGawDo6UTKDc"
      },
      "source": [
        "**('REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION') 0.86**  \n",
        "**('REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY') 0.83**\n",
        "\n",
        "Il s'agit ici de vérifier, au niveau de la région ou de la ville, si l'adresse de travail de l'emprunteur est différente de son adresse de résidence ou de son adresse de contact. Notons que les corrélations entre les variables croisées, par exemple entre REG_REGION_NOT_WORK_REGION et REG_CITY_NOT_WORK_CITY, sont inférieures à 0.7 puisqu'elles ne ressortent pas dans le détail ci-dessus. \n",
        "On peut aussi relever là encore la différence entre un modèle américan et français puisque le premier accepte une troisième adresse en plus de celle du domicile et celle du travail : en France, l'adresse du domicile est obligatoirement l'adresse de contact, une autre adresse ne serait tout simplement pas prise en compte.\n",
        "\n",
        "On retient ici les variables LIVE_ et on supprime les variables REG_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tavt6XvdTKDd"
      },
      "source": [
        "data = data.drop(['REG_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_WORK_CITY'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7onnVcsVTKDd"
      },
      "source": [
        "**('FLOORSMAX_AVG', 'FLOORSMIN_AVG') : 0.74**  \n",
        "Ces données manquent de clarté : le nombre maximum d'étages concerne-t-il le logement de l'emprunteur (auquel cas c'est tout simplement le nombre d'étages) ou le type de logement qu'il occupe ? Or dans ce cas il ne s'agit pas d'une donnée qui lui est personnelle et donc susceptible d'être utile au modèle. Pour diminuer le nombre de variables on retiendra la moyenne de ces deux valeurs. On sera par ailleurs attentif à la pertinence de la variable dans le modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI5Zdm_pTKDe"
      },
      "source": [
        "data['FLOORS_AVG'] = (data['FLOORSMAX_AVG'] + data['FLOORSMIN_AVG'])/2\n",
        "data.drop(['FLOORSMAX_AVG','FLOORSMIN_AVG'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DllIVNpzTJ5g"
      },
      "source": [
        "def analyse_num_with_target_bin(positive_df,negative_df,target_var,remove=[\"Col_ID\"]):\n",
        "    num_cols = [col for col in positive_df.columns if not col in positive_df.select_dtypes([\"object\",\"category\"]).columns.to_list()]\n",
        "    if len(remove) > 0:\n",
        "        num_cols = [col for col in num_cols if (col not in remove)]\n",
        "        \n",
        "    for col in num_cols:\n",
        "        fig,axes = plt.subplots(1,3,figsize=(25,6))\n",
        "        sns.distplot(positive_df[col],label=\"positive\",ax=axes[0])\n",
        "        sns.distplot(negative_df[col],label=\"negative\",ax=axes[0])\n",
        "\n",
        "        pd.concat([positive_df,negative_df],axis=0).boxplot(str(col),by=target_var,ax=axes[1],vert=False)\n",
        "        \n",
        "        pd.concat([positive_df,negative_df],axis=0).boxplot(str(col),ax=axes[2],vert=False)\n",
        "\n",
        "        axes[0].legend()\n",
        "        # Taille max du nom de la colonne 50\n",
        "        axes[0].set_title(col[:50] + \" | Distplot\")\n",
        "        axes[1].set_title(col[:50] + \" | Boxplot split\")\n",
        "        axes[2].set_title(col[:50] + \" | Boxplot global\")\n",
        "        \n",
        "        axes[0].set_xlabel(\"\")\n",
        "        axes[1].set_xlabel(\"\")\n",
        "        axes[2].set_xlabel(\"\")\n",
        "        \n",
        "        axes[2].set_yticklabels([])\n",
        "        axes[2].set_yticks([])\n",
        "        \n",
        "\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG-yAgHNlYh5"
      },
      "source": [
        "new_col = [\"CREDIT_INCOME_RATIO\",\"CREDIT_GOOD_RATIO\",\"FLOORS_AVG\"]\n",
        "positive_target = data[data[\"TARGET\"] == 1].sample(20000,random_state=0)\n",
        "negative_target = data[data[\"TARGET\"] == 0].sample(20000,random_state=0)\n",
        "analyse_num_with_target_bin(positive_target,negative_target,\"TARGET\",remove=[col for col in data.columns if not col in new_col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJibrCnKTKDe"
      },
      "source": [
        "Affichons la matrice de corrélation actualisée :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgqzXSzyTKDe"
      },
      "source": [
        "show_correlation_matrix(data.corr())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu1qssKuTKDf"
      },
      "source": [
        "list_high_correlations(data, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISOCSqqkTKDf"
      },
      "source": [
        "APARTMENTS_AVG est fortement corrélé à 3 variables : ENTRANCES_AVG, BASEMENTAREA_AVG et FLOORS_AVG, on supprime celles-ci."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8lVW5NGTKDg"
      },
      "source": [
        "data.drop(['ENTRANCES_AVG', 'BASEMENTAREA_AVG', 'FLOORS_AVG'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3VzrN7aTKDg"
      },
      "source": [
        "list_high_correlations(data, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Il2xaeETKDh"
      },
      "source": [
        "Les corrélations ne semblent pas suffisantes pour autoriser une modification des variables. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7djQoiVlTKDh"
      },
      "source": [
        "Vérifions les données après nettoyage :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A6vTL8XTKDh"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_8tfKgXTKDi"
      },
      "source": [
        "On intègre les opérations réalisées ci-dessus à la fonction `clean_features()` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4YDmxuUTKDi"
      },
      "source": [
        "def clean_features(df):\n",
        "    print('Nettoyage des colonnes...')\n",
        "    # Drop columns with a single value\n",
        "    \n",
        "    corr = df.corr()\n",
        "    constant_cols = corr[corr['AMT_INCOME_TOTAL'].isna()].index.to_list()\n",
        "    df.drop(constant_cols, axis=1, inplace=True)\n",
        "    \n",
        "    # Drop MODE and MEDI columns \n",
        "    \n",
        "    cols_without_mode_and_medi = [col for col in df.columns \n",
        "                                     if not (col.endswith('_MODE') or col.endswith('_MEDI'))]\n",
        "    df = df[cols_without_mode_and_medi]\n",
        "    \n",
        "    # Create new synthetic features\n",
        "    \n",
        "    df['CREDIT_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
        "    df['CREDIT_GOOD_RATIO'] = df['AMT_CREDIT']/(df['AMT_GOODS_PRICE']+1)\n",
        "    df['FLOORS_AVG'] = (df['FLOORSMAX_AVG'] + df['FLOORSMIN_AVG'])/2\n",
        "    \n",
        "    # Drop redundant columns\n",
        "    \n",
        "    cols_to_drop = ['FLAG_EMP_PHONE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
        "                   'AMT_ANNUITY','AMT_CREDIT', 'AMT_GOODS_PRICE', 'REGION_RATING_CLIENT_W_CITY',\n",
        "                   'ELEVATORS_AVG', 'LIVINGAPARTMENTS_AVG','LIVINGAREA_AVG','CNT_FAM_MEMBERS',\n",
        "                    'REG_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_WORK_CITY',\n",
        "                   'FLOORSMAX_AVG','FLOORSMIN_AVG',\n",
        "                    'ENTRANCES_AVG', 'BASEMENTAREA_AVG', 'FLOORS_AVG']\n",
        "    \n",
        "    df = df.drop(cols_to_drop, axis=1)   \n",
        "        \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7ljW6yVTKDj"
      },
      "source": [
        "cleaned_values_data = clean_values(application_train)\n",
        "data = clean_features(cleaned_values_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB0EwYJuTKDj"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBe8at5PTKDk"
      },
      "source": [
        "features_cols = [col for col in data.columns if col not in ['SK_ID_CURR', 'TARGET']]\n",
        "X = data[features_cols]\n",
        "y = data['TARGET']\n",
        "id = data['SK_ID_CURR']\n",
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUUgel4QwQiS"
      },
      "source": [
        "X_train,X_test,y_train,y_test = repartition_dataset(data[features_cols],\"TARGET\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6KOiHH3402n"
      },
      "source": [
        "La standardisation sera réalisée avec StandardScaler, l'encodage par One Hot Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-06-10T12:48:22.048348Z",
          "iopub.execute_input": "2021-06-10T12:48:22.048699Z",
          "iopub.status.idle": "2021-06-10T12:48:22.060697Z",
          "shell.execute_reply.started": "2021-06-10T12:48:22.048671Z",
          "shell.execute_reply": "2021-06-10T12:48:22.058927Z"
        },
        "trusted": true,
        "id": "rMEAUCWC402q"
      },
      "source": [
        "num_cols = [col for col in X_train.columns if data[col].dtype != 'object']\n",
        "cat_cols = [col for col in X_train.columns if col not in num_cols]\n",
        "\n",
        "transformer = make_column_transformer(\n",
        "    (StandardScaler(), num_cols),\n",
        "    (OneHotEncoder(), cat_cols)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNIoP6EC402r"
      },
      "source": [
        "On vérifie que le transformer produit le résultat attendu (sans modifier X) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-06-10T12:48:22.062339Z",
          "iopub.execute_input": "2021-06-10T12:48:22.062659Z",
          "iopub.status.idle": "2021-06-10T12:48:37.847813Z",
          "shell.execute_reply.started": "2021-06-10T12:48:22.062629Z",
          "shell.execute_reply": "2021-06-10T12:48:37.846660Z"
        },
        "trusted": true,
        "id": "5rM9gyEq402r"
      },
      "source": [
        "result = transformer.fit_transform(X_train)\n",
        "transformed_cols = num_cols + list(transformer.named_transformers_['onehotencoder'].get_feature_names())\n",
        "df_result = pd.DataFrame(result, columns=transformed_cols)\n",
        "print(df_result.shape)\n",
        "df_result.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDEjDJnQTKDl"
      },
      "source": [
        "### 4.2 Essai avec variables modifiées"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x0DHTdWTKDl"
      },
      "source": [
        "On relance le modèle après avoir actualisé le transformer chargé d'encoder X puisque les variables ont changé : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz86gDvATKDl"
      },
      "source": [
        "num_cols = [col for col in X_train.columns if data[col].dtype != 'object']\n",
        "cat_cols = [col for col in X_train.columns if col not in num_cols]\n",
        "\n",
        "transformer = make_column_transformer(\n",
        "    (StandardScaler(), num_cols),\n",
        "    (OneHotEncoder(), cat_cols)\n",
        "    )\n",
        "\n",
        "pipeline = make_pipeline(transformer, DecisionTreeClassifier())\n",
        "params = {'decisiontreeclassifier__max_depth': [None]}\n",
        "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
        "accuracy_scorer = make_scorer(accuracy_score)\n",
        "\n",
        "grid_dtc_n = GridSearchCV(\n",
        "    pipeline, \n",
        "    param_grid=params, \n",
        "    cv=5, \n",
        "    scoring={'acc': accuracy_scorer, 'ftwo': ftwo_scorer},\n",
        "    refit='ftwo', \n",
        "    verbose=2\n",
        ")\n",
        "_= grid_dtc_n.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a46QLL_w32k"
      },
      "source": [
        "f2_dtc_n = evaluation(grid_dtc_n,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4W1yIbSXECC"
      },
      "source": [
        "plot_all_roc_curve({\"Decision Tree New Var\":grid_dtc_n},X_test,y_test,naive_model=dummy_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfUsxrW3TKDm"
      },
      "source": [
        "update_recap('dtc nouvelles variables', \n",
        "             grid_dtc_n.cv_results_['mean_test_acc'][0], \n",
        "             grid_dtc_n.cv_results_['mean_test_ftwo'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf7e8HimTKDn"
      },
      "source": [
        "Le traitement des variables n'améliore pas la performance du modèle mais la temps de execution est réduit à moitié"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_YVBy1VTKDn"
      },
      "source": [
        "### 4.3 Essai avec régression logistique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XySRC58TKDo"
      },
      "source": [
        "Essayons un autre modèle, la régression logistique :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJRB_oRsTKDo"
      },
      "source": [
        "pipeline = make_pipeline(transformer, LogisticRegression())\n",
        "params = {'logisticregression__C': [.1],\n",
        "         'logisticregression__max_iter': [10],\n",
        "         'logisticregression__solver': ['saga']}\n",
        "\n",
        "grid_lr = GridSearchCV(\n",
        "    pipeline, \n",
        "    param_grid=params, \n",
        "    cv=5, \n",
        "    scoring={'acc': accuracy_scorer, 'ftwo': ftwo_scorer},\n",
        "    refit='ftwo', \n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjLik4LjTKDo"
      },
      "source": [
        "_ = grid_lr.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfchGevo1CqN"
      },
      "source": [
        "f2_lr = evaluation(grid_lr,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhFEAsU_Wjs3"
      },
      "source": [
        "# plot_all_roc_curve({\"logistique régression\":grid_lr},X_test,y_test,naive_model=grid_dtc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpISHDaSTKDp"
      },
      "source": [
        "update_recap('régression logistique',\n",
        "             grid_lr.cv_results_['mean_test_acc'][0], \n",
        "             grid_lr.cv_results_['mean_test_ftwo'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckIN07l-TKDq"
      },
      "source": [
        "grid_lr.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P6F-s4_IKgZ"
      },
      "source": [
        "### 4.4 Essai avec Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIXWvtkGHDuO"
      },
      "source": [
        "pipeline = make_pipeline(transformer, RandomForestClassifier())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "808B1JyoHFpa"
      },
      "source": [
        "# sorted(pipeline.get_params().keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMF1SvtTEd3Z"
      },
      "source": [
        "\n",
        "params = {'randomforestclassifier__n_estimators': [250],\n",
        "          'randomforestclassifier__class_weight': ['balanced'],\n",
        "          'randomforestclassifier__criterion': [\"entropy\"],\n",
        "          'randomforestclassifier__max_depth': [10],\n",
        "          'randomforestclassifier__n_jobs': [50],\n",
        "          'randomforestclassifier__random_state': [0]\n",
        "          }\n",
        "\n",
        "grid_rf = GridSearchCV(\n",
        "    pipeline, \n",
        "    param_grid=params, \n",
        "    cv=5, \n",
        "    scoring={'acc': accuracy_scorer, 'ftwo': ftwo_scorer},\n",
        "    refit='ftwo', \n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ii_F2uWEnEM"
      },
      "source": [
        "_ = grid_rf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rx67hZGMgdr"
      },
      "source": [
        "f2_rf = evaluation(grid_rf,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rx4vEeFUTK6"
      },
      "source": [
        "plot_all_roc_curve({\"Random Forest\":grid_rf, \"logistique régression\":grid_lr, \"Decision Tree New Var\":grid_dtc_n},X_test,y_test,naive_model=dummy_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7CNDo63RDS7"
      },
      "source": [
        "update_recap('Random Forest',\n",
        "             grid_rf.cv_results_['mean_test_acc'][0], \n",
        "             grid_rf.cv_results_['mean_test_ftwo'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBpcVWGzSDtO"
      },
      "source": [
        "grid_rf.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvwjjb46pYNw"
      },
      "source": [
        "grid_rf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97c9f53"
      },
      "source": [
        "## Entrainement sur tout le jeu de données\n",
        "\n",
        "Maintenant que nous avons un modèle suffisamment performant, nous allons l'entraîner sur la totalité du jeu de données pour permettre de faire des prédictions les plus fiables possibles sur des données extérieures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fc67a21"
      },
      "source": [
        "best_model = RandomForestClassifier(n_estimators=250,\n",
        "                                    criterion=\"entropy\",\n",
        "                                    max_depth=10,\n",
        "                                    class_weight=\"balanced\",\n",
        "                                    random_state=0,n_jobs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af07181b"
      },
      "source": [
        "feature_names = [i for i in X_train.columns if X_train[i].dtype in [np.float64, np.float32, np.int64, np.int32]]\n",
        "df_features = X_train[feature_names]\n",
        "print(\"Shape X :\",df_features.shape)\n",
        "print(\"Shape y :\",y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jEBjX3QbvV3"
      },
      "source": [
        "feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61d65360"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "print(\"Score CV :\",cross_val_score(best_model,df_features,y_train,scoring=ftwo_scorer,cv=5).mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcfc8f5f"
      },
      "source": [
        "best_model.fit(df_features,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dc2f726"
      },
      "source": [
        "**Le modèle est prêt à être utilisé !**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf2ed473"
      },
      "source": [
        "# Explication du modèle\n",
        "\n",
        "Dans cette partie, nous allons étudier le comportement du modèle pour expliquer sa prise de décision sur les individus, pour ça, nous allons faire ressortir les paramètres les plus marquants dans la prise de décision du modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fb287f1"
      },
      "source": [
        "def get_importance_feature(model,feature,sort=True,n_first=None):\n",
        "    if  hasattr(model,\"coef_\"):\n",
        "        coef = model.coef_.tolist()[0]\n",
        "    elif hasattr(model,\"feature_importances_\"):\n",
        "         coef = model.feature_importances_.tolist()    \n",
        "    importance_feature = {}\n",
        "    for i in range(len(feature)):\n",
        "        importance_feature[feature[i]]=abs(coef[i])\n",
        "    \n",
        "    if sort:\n",
        "        importance_feature = dict(sorted(importance_feature.items(), key=lambda item: item[1],reverse=True))\n",
        "    \n",
        "    if n_first!=None:\n",
        "        importance_feature={list(importance_feature.keys())[i]:list(importance_feature.values())[i] for i in range(len(importance_feature)) if i < n_first }\n",
        "    \n",
        "    return importance_feature\n",
        "    \n",
        "def get_best_param_model(importance_feature,figsize=(20,7),log=False):\n",
        "    feature = list(importance_feature.keys())\n",
        "    coef = list(importance_feature.values())\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.title(\"Importance des variables\")\n",
        "    plt.bar(feature,coef)\n",
        "    plt.ylabel(\"coef attribué par le modèle\")\n",
        "    if log:\n",
        "        plt.yscale(\"log\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75ff7f45"
      },
      "source": [
        "get_best_param_model(get_importance_feature(best_model,df_features.columns,n_first=20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7e22953"
      },
      "source": [
        "## Explication SHAP \n",
        "\n",
        "\n",
        "SHAP : SHapley Additive ExPlanations\n",
        "\n",
        "SHAP permet lui d'expliquer l'impact des variables sur le modèle ou la prédiction d'un client par rapport aux différentes variables, il permet également de voir l'impact global du jeu de données des différentes variables.\n",
        "\n",
        "Cet approche donnent un meilleur aperçu des prédictions du modèle et l'impact des variables sur les prédictions, elles permettent également de voir à quel dégrée les variables influences une décision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b189e187"
      },
      "source": [
        "### SHAP :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPUFnElyQLpw"
      },
      "source": [
        "# !pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbc70b1f"
      },
      "source": [
        "import shap\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "shap.initjs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcc0e258"
      },
      "source": [
        "shap_explainer = shap.TreeExplainer(best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "084d7c67"
      },
      "source": [
        "%%time\n",
        "shap_values = shap_explainer.shap_values(df_features[:10000]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "108cb7cf"
      },
      "source": [
        "print('Expected Value:', shap_explainer.expected_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60dffbc0"
      },
      "source": [
        "shap.initjs()\n",
        "shap.force_plot(shap_explainer.expected_value[1], \n",
        "                shap_values[1][0,:], df_features.iloc[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36241fb8"
      },
      "source": [
        "shap.initjs()\n",
        "shap.force_plot(shap_explainer.expected_value[1], \n",
        "                shap_values[1][1,:], df_features.iloc[1,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c458958a"
      },
      "source": [
        "# Impact pour le non remboursement class 1\n",
        "shap.initjs()\n",
        "shap.force_plot(shap_explainer.expected_value[1], \n",
        "                shap_values[1][:1000,:], df_features.iloc[:1000,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50c89959"
      },
      "source": [
        "Ce graphique nous donne de multiples informations, il permet de voir les effets des décisions par rapport aux différentes variables, si on sélectionne par exemple **les ressources extérieures**, on remarque par exemple que **plus le score de ces variables est faible plus la chance de non-remboursement est élevée**. \n",
        "\n",
        "On peut également analyser toutes les autres variables, par exemple le **'DAYS_EMPLOYED'** on voit que les personnes qui ont **plus de 5 ans d'ancienneté ont plus de chance de rembourser leur crédit !**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d949539"
      },
      "source": [
        "shap.summary_plot(shap_values,df_features[:10000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f60bff8b"
      },
      "source": [
        "Globalement, il est difficile de dire quelle variable impact plus une classe qu'une autre, c'est pour ça qu'il est important de voir les variantes pour la prédiction pour chaque variable. ( On choisira ici les plus impactant comme les EXT_SOURCE_{1,2,3} , DAYS_EMPLOYED, DAYS_BIRTH...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e791fbbb"
      },
      "source": [
        "shap.dependence_plot('DAYS_EMPLOYED', shap_values[1], df_features[:10000],interaction_index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7db3df6"
      },
      "source": [
        "Ici on voit que à partir d'environ 5 ans d'ancienneté la chance de rembourser le prêt et de plus en plus important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a7e4739"
      },
      "source": [
        "shap.dependence_plot('DAYS_BIRTH', shap_values[1], df_features[:10000],interaction_index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9240e7de"
      },
      "source": [
        "Pour l'âge, au-dessus de 41 ans, il semble que les individus aient plus de chance de rembourser le prêt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cff8bf5b"
      },
      "source": [
        "shap.dependence_plot('EXT_SOURCE_1', shap_values[1], df_features[:10000],interaction_index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2cf18e7"
      },
      "source": [
        "shap.dependence_plot('EXT_SOURCE_2', shap_values[1], df_features[:10000],interaction_index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e94c542c"
      },
      "source": [
        "shap.dependence_plot('EXT_SOURCE_3', shap_values[1], df_features[:10000],interaction_index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13a133e3"
      },
      "source": [
        "Pour les ressources extérieures, la limite pour les ressources:\n",
        "- 1 est à environ 0.35\n",
        "- 2 est à environ 0.45\n",
        "- 3 est à environ 0.45\n",
        "\n",
        "En dessous de ces limites, les clients ont du mal à rembourser leur crédit"
      ]
    }
  ]
}
